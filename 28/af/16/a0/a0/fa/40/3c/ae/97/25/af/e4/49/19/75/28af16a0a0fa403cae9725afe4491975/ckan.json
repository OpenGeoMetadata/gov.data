{
  "license_title": "U.S. Government Work",
  "maintainer": "John Stutz",
  "relationships_as_object": [

  ],
  "private": false,
  "maintainer_email": "john.c.stutz@nasa.gov",
  "num_tags": 3,
  "id": "28af16a0-a0fa-403c-ae97-25afe4491975",
  "metadata_created": "2016-02-26T21:45:33.557100",
  "metadata_modified": "2016-05-06T08:18:34.384149",
  "author": null,
  "author_email": null,
  "state": "active",
  "version": null,
  "creator_user_id": "47303a9e-1187-4290-85a3-1fc02dc49e4a",
  "type": "dataset",
  "resources": [

  ],
  "num_resources": 0,
  "tags": [
    {
      "vocabulary_id": null,
      "state": "active",
      "display_name": "ames",
      "id": "26409e70-decd-4cc6-9beb-904602430852",
      "name": "ames"
    },
    {
      "vocabulary_id": null,
      "state": "active",
      "display_name": "dashlink",
      "id": "4377f23e-7f40-493e-82fd-834ccb75e9ba",
      "name": "dashlink"
    },
    {
      "vocabulary_id": null,
      "state": "active",
      "display_name": "nasa",
      "id": "34817959-c506-4bfe-b8fd-330f550970e4",
      "name": "nasa"
    }
  ],
  "tracking_summary": {
    "total": 8,
    "recent": 1
  },
  "groups": [

  ],
  "license_id": "us-pd",
  "relationships_as_subject": [

  ],
  "organization": {
    "description": "NASA's vision: To reach for new heights and reveal the unknown so that what we do and learn will benefit all humankind.\r\n\r\nTo do that, thousands of people have been working around the world -- and off of it -- for 50 years, trying to answer some basic questions. What's out there in space? How do we get there? What will we find? What can we learn there, or learn just by trying to get there, that will make life better here on Earth?\r\n\r\nNASA conducts its work in four principal organizations, called mission directorates:\r\n \r\nAeronautics: works to solve the challenges that still exist in our nation's air transportation system: air traffic congestion, safety and environmental impacts.\r\n\r\nHuman Exploration and Operations: focuses on International Space Station operations, development of commercial spaceflight opportunities and human exploration beyond low Earth orbit.\r\n\r\nScience: explores the Earth, solar system and universe beyond; charts the best route of discovery; and reaps the benefits of Earth and space exploration for society.\r\n\r\nSpace Technology: rapidly develops, demonstrates, and infuses revolutionary, high-payoff technologies, expanding the boundaries of the aerospace enterprise.\r\n\r\nIn the early 21st century, NASA's reach spans the universe. The Mars rover Curiosity is still exploring Mars to see if it might once have had environments suitable for life. Cassini is in orbit around Saturn, as Juno makes its way to Jupiter. The restored Hubble Space Telescope continues to explore the deepest reaches of the cosmos as NASA developes the James Webb Space Telescope.\r\n\r\nCloser to home, the latest crew of the International Space Station is extending the permanent human presence in space. With commercial partners such as SpaceX, NASA is helping to foster the development of private-sector aerospace.\r\n\r\nEarth science satellites are sending back unprecedented data on Earth's oceans, climate and other features. NASA's aeronautics team is working with other government organizations, universities, and industry to fundamentally improve the air transportation experience and retain our nation's leadership in global aviation.",
    "created": "2013-03-14T03:44:44.467521",
    "title": "National Aeronautics and Space Administration",
    "name": "nasa-gov",
    "is_organization": true,
    "state": "active",
    "image_url": "http://polargateways2008.gsfc.nasa.gov/images/LOGO_2in/NASA_2.jpg",
    "revision_id": "7f05110b-681a-402c-a372-74500dc583dd",
    "type": "organization",
    "id": "d8a6202b-c0c3-463d-88a0-3bc021a178ed",
    "approval_status": "approved"
  },
  "name": "efficient-matlab-programs",
  "isopen": true,
  "url": null,
  "notes": "Matlab has a reputation for running slowly.  Here are some pointers on how to speed computations, to an often unexpected degree.  Subjects currently covered:\r\n\r\n \r\n\r\nMatrix Coding\r\n\r\nImplicit Multithreading on a Multicore Machine\r\n\r\nSparse Matrices\r\n\r\nSub-Block Computation to Avoid Memory Overflow\r\n\r\n --------------------------------------------------------------------------------------------------------\r\n\r\n \r\n\r\nMatrix Coding - 1\r\n\r\n \r\n\r\n Matlab documentation notes that efficient computation depends on using the matrix facilities, and that mathematically identical algorithms can have very different runtimes, but they are a bit coy about just what these differences are.  A simple but telling example:\r\n\r\n \r\n\r\n The following is the core of the GD-CLS algorithm of Berry et.al., copied from fig. 1 of Shahnaz et.al, 2006, \"Document clustering using nonnegative matrix factorization':\r\n\r\n         for jj = 1:maxiter\r\n             A = W'*W + lambda*eye(k);\r\n             for ii = 1:n\r\n                 b = W'*V(:,ii);\r\n                 H(:,ii) = A \\ b;\r\n             end\r\n             H = H .* (H>0);\r\n             W = W .* (V*H') ./ (W*(H*H') + 1e-9);\r\n         end\r\n\r\n \r\n\r\nReplacing the columwise update of H with a matrix update gives:\r\n\r\n          for jj = 1:maxiter\r\n              A = W'*W + lambda*eye(k);\r\n              B = W'*V;\r\n              H = A \\ B;\r\n              H = H .* (H>0);\r\n              W = W .* (V*H') ./ (W*(H*H') + 1e-9);\r\n          end\r\n\r\n \r\n\r\nThese were tested on an 8049 x 8660 sparse matrix bag of words V (.0083 non-zeros), with W of size 8049 x 50, H 50 x 8660, maxiter = 50, lambda = 0.1, and identical initial W.   They were run consecutivly, multithreaded on an 8-processor Sun server, starting at ~7:30PM.  Tic-toc timing was recorded. \r\n\r\n \r\n\r\nRuntimes were respectivly  6586.2 and 70.5 seconds, a 93:1 difference.  The maximum absolute pairwise difference between W matrix values was 6.6e-14.  \r\n\r\n \r\n\r\nSimilar speedups have been consistantly observed in other cases.  In one algorithm, combining matrix operations with efficient use of the sparse matrix facilities gave a 3600:1 speedup. \r\n\r\n \r\n\r\nFor speed alone, C-style iterative programming should be avoided wherever possible.  In addition,  when a couple lines of matrix code can substitute for an entire C-style function, program clarity is much improved. \r\n\r\n----------------------------------------------------------------------------------------------------------------------\r\n\r\n Matrix Coding - 2\r\n\r\n \r\n\r\nApplied to integration, the speed gains are not so great,  largely due to the time taken to set up the and deal with the boundaries.  The anyomous function setup time is neglegable.  I demonstrate on a simple uniform step linearly interpolated 1-D integration of cos() from 0 to pi, which should yield zero:\r\n          tic;\r\n          step = .00001;\r\n          fun = @cos;\r\n          start = 0;\r\n          endit = pi;\r\n          enda = floor((endit - start)/step)*step + start;\r\n          delta = (endit - enda)/step;\r\n          intF = fun(start)/2;\r\n          intF = intF + fun(endit)*delta/2;\r\n          intF = intF + fun(enda)*(delta+1)/2;\r\n          for ii = start+step:step:enda-step\r\n             intF = intF + fun(ii);\r\n          end\r\n          intF = intF*step\r\n          toc;\r\n\r\nintF =   -2.910164109692914e-14\r\nElapsed time is 4.091038 seconds.\r\n\r\n \r\n\r\nReplacing the inner summation loop with the matrix equivalent speeds things up a bit:\r\n\r\n          tic;\r\n          step = .00001;\r\n          fun = @cos;\r\n          start = 0;\r\n          endit = pi;\r\n          enda = floor((endit - start)/step)*step + start;\r\n          delta = (endit - enda)/step;\r\n          intF = fun(start)/2;\r\n          intF = intF + fun(endit)*delta/2;\r\n          intF = intF + fun(enda)*(delta+1)/2;\r\n          intF = intF + sum(fun(start+step:step:enda-step));\r\n          intF = intF*step\r\n          toc;\r\nintF =   -2.868419946011613e-14\r\nElapsed time is 0.141564 seconds.\r\n\r\n \r\n\r\nThe core computation takes about 70% of the full time used above, but without the boundary handling gives a potentially significant error:\r\n\r\n        tic; sum(fun(start:step:endit))*step,  toc;\r\nans =    2.653589763815900e-06\r\nElapsed time is 0.099088 seconds. ---------------------------------------------------------------------------------------------------------------------- \r\n\r\nImplicit Multithreading on a Multicore Machine\r\n\r\n \r\n\r\nSelect File > Preferences > General > Multithreading. and enable multithreading.   Enjoy.\r\n\r\n \r\n\r\n See  Multiprocessing in MATLAB on the help system for documentation.\r\n\r\n-----------------------------------------------------------------------------------------------------------------------\r\n\r\n Sparse Matrices \r\n\r\n \r\n\r\nIn Matlab, the sparse matrix facilities are activated by simply making a matrix sparse,as in  Xs = sparse(X), and using it.  Most of the matrix operations are able to make effecient use of truely sparse matrices, with considerable relief of memory demands.  A simple example follows: \r\n\r\n \r\n\r\n load ... /home/stutz/DaSH/IDU/ASAP/Runs-20080303_gdcls/gdcls_dataset.mat\r\n\r\nX = Data0.X_T;       % a terms*docs bag of word counts\r\nnnz(X) / numel(X)\r\n% ans = 0.0044\r\n\r\nC = Data0.Cats;      % a 0/1 valued non-exclusive category matrix\r\nnnz(C) / numel(C)\r\n% ans =  0.0329\r\n\r\nThe memory saved by using sparse matrices is not trivial:\r\n\r\nXf = full(X);\r\nCf = full(C);\r\nCt = C';\r\nCtf = Cf';\r\nXt = X';\r\nXtf = Xf';\r\n\r\nwhos X Xt Xf Xtf C Ct Cf Ctf\r\n\r\n%   Name          Size                    Bytes  Class     Attributes\r\n%\r\n%   C         11245x33                   195536  double  sparse \r\n\r\n%   Cf        11245x33                2968680  double              \r\n%   Ct        33x11245                   285232  double  sparse   \r\n%   Ctf       33x11245                2968680  double              \r\n%   X      4686x11245             11649776  double  sparse \r\n%   Xf   14686x11245        1321152560  double              \r\n%   Xt   11245x14686            11677304  double  sparse \r\n%   Xtf  11245x14686       1321152560  double  \r\n\r\n \r\n\r\nThe disparity between C and Ct byte counts are due  to the assymetry.  Matlab's sparse representation requires a column index for non-empty columns plus a row index for each non-zero element. \r\n\r\n\r\nThe matrix multiplication timings vary significantly, but not quite as one might expect, due to the great disparity in matrix sizes:\r\n\r\ntic;  X*C;  toc;     % sparse*sparse\r\n%  Elapsed time is 0.100225, 0.108354, 0.115247, 0.102686 seconds.\r\n\r\ntic;  Ct*Xt;  toc;   % sparse*sparse\r\n%  Elapsed time is 0.153515, 0.146475, 0.145449, 0.156352  seconds.\r\n\r\ntic; Ctf*Xt; toc;    % full*sparse\r\n%  Elapsed  0.404232, 0.421417, 0.424837, 0.404256\r\n\r\ntic;  X*Cf; toc;     % sparse*full\r\n%  Elapsed 0.762932, 1.148991, 0.727793, 0.717451\r\n\r\ntic; Ctf*Xtf; toc;   % full*full\r\n%  Elapsed 2.984380, 3.164782, 3.050478, 3.253079\r\n\r\ntic; Xf*Cf; toc;     % full*full\r\n%  Elapsed 3.423657, 3.503390, 3.854899, 3.555406\r\n\r\ntic; Xf*C;  toc;     % full*sparse\r\n%  Elapsed 7.204459, 7.193888, 8.999477, 9.477529\r\n\r\ntic;  Ct*Xtf;  toc;  % sparse*full\r\n%  Elapsed 9.655192, 7.155294, 7.155073, 7.496134\r\n\r\nThe above was monitored with UNIX top, while running basic Matlab in multithreading mode on an 8-processor server.  These calculations involved up to 16% of CPU, say 1.3 processors, but typically 12.5% or 1 processor, dropping to under 0.04% when idle.\r\n\r\nMatrix normalization time differences are even greater, depite the advantage of multithreading  The sparse matrix normalizations below ran with no more than 12.5% CPU.  The following full matrix calculations quickly ramped up and held at 90-91.8% of CPU, say 7.3 processors, with only one significant (>0.2% CPU) competing process present.\r\n\r\n \r\n\r\ntic;  X / spdiags(sum( X.^2)', 0, 11245, 11245); toc;\r\n\r\n% sparse/sparse(sparse)\r\n% Elapsed32.781410, 30.889889, 30.978490, 32.140246,\r\n\r\nic;  X / diag(sum(X.^2)); toc;             % sparse/full(sparse)\r\n%  Elapsed 30.714821, 30.973805, 30.398733, 30.443698,\r\n\r\ntic; Xf / diag(sum(Xf.^2)); toc;                      % full/full(full)\r\n\r\nKilled Matlab (not responding) after 22 hours weekend clock time.\r\n\r\ntic; Xf / spdiags(sum(Xf.^2)', 0, 11245, 11245); toc; %full/sparse(full)\r\n\r\nKilled Matlab (not responding) after 42 hours weekend clock time.\r\n-----------------------------------------------------------------------------------------------------------------------\r\n\r\n \r\n\r\nSub-Block Computation to Avoid Memory Overflow \r\n\r\n \r\n\r\nThe downside of matrix  computation is that intermediate values may be so large that the total memory allocation exceeds what is available. Using sparse matrices helps avoid this, but not all of our matrices are sparse.  If a calculation can be done on a column by colume basis, or line by line, or page by page..., we can usually break it into blocks of computable size while retaining most of speed advantage of matrix calculation.  Consider the following schematic:\r\n\r\n\r\nfunction [...] = whatever(.....)\r\ntry\r\n    <the basic matrix calculation>\r\n    return;  % straight matrix result\r\n\r\ncatch\r\n    <set a large initial blocksize>\r\n    try\r\n        <set up accumulator(s)>\r\n        while blocksize >= 1\r\n            try\r\n                <zero accumulator(s)>\r\n                <set up sub-blocks and bookkeeping>\r\n                for <loop over sub-blocks>\r\n                    <do calculation on a sub-block)\r\n                    <add result to accumulator>\r\n                end\r\n              return  % accumulated result\r\n          catch\r\n              blocksize = floor(blocksize / 2);\r\n          end % inner try\r\n      end % while\r\n      error('unable to complete whatever at blocksize == 1');\r\n    catch % middle try\r\n        error('unable to allocate accumulator(s)');\r\n    end % middle try\r\nend % outer try\r\n \r\n\r\nThe initially tried matrix calculation both lets us deal effeciently with small matrices and provides good documentation for what we do in the more complex blocked calculation.  The intermediate try allows a graceful exit when we cannot even build an accumulator, and is probably not needed when our result is of smaller dimension than the inputs.  The inner try/while quickly runs our blocksize down to something that that works, so long as the initial block processed is one of the largest. \r\n\r\n -------------------------------------------------------------------------------------------",
  "owner_org": "d8a6202b-c0c3-463d-88a0-3bc021a178ed",
  "extras": [
    {
      "key": "harvest_source_id",
      "value": "39e4ad2a-47ca-4507-8258-852babd0fd99"
    },
    {
      "key": "issued",
      "value": "2010-09-09T16:27:10"
    },
    {
      "key": "resource-type",
      "value": "Dataset"
    },
    {
      "key": "source_hash",
      "value": "739ef823f3e8cf3d0963740e806d84ed4c39c4b9"
    },
    {
      "key": "accessLevel",
      "value": "public"
    },
    {
      "key": "catalog_conformsTo",
      "value": "https://project-open-data.cio.gov/v1.1/schema"
    },
    {
      "key": "catalog_@context",
      "value": "https://project-open-data.cio.gov/v1.1/schema/catalog.jsonld"
    },
    {
      "key": "catalog_describedBy",
      "value": "https://project-open-data.cio.gov/v1.1/schema/catalog.json"
    },
    {
      "key": "harvest_source_title",
      "value": "NASA Data.json"
    },
    {
      "key": "source_schema_version",
      "value": "1.1"
    },
    {
      "key": "programCode",
      "value": [
        "026:029"
      ]
    },
    {
      "key": "bureauCode",
      "value": [
        "026:00"
      ]
    },
    {
      "key": "landingPage",
      "value": "https://c3.nasa.gov/dashlink/resources/9/"
    },
    {
      "key": "harvest_object_id",
      "value": "f578cd99-3199-47da-aaf5-c8359f48a6e8"
    },
    {
      "key": "publisher",
      "value": "Dashlink"
    },
    {
      "key": "_id",
      "value": {
        "$oid": "56cf5b00a759fdadc44e55cd"
      }
    },
    {
      "key": "license",
      "value": "http://www.usa.gov/publicdomain/label/1.0/"
    },
    {
      "key": "language",
      "value": [
        "en-US"
      ]
    },
    {
      "key": "publisher_hierarchy",
      "value": "U.S. Government > National Aeronautics and Space Administration > Dashlink"
    },
    {
      "key": "modified",
      "value": "2010-09-29T15:18:32"
    },
    {
      "key": "source_datajson_identifier",
      "value": true
    },
    {
      "key": "accrualPeriodicity",
      "value": "irregular"
    },
    {
      "key": "identifier",
      "value": "DASHLINK_9"
    },
    {
      "key": "harvest_object_id",
      "value": "f578cd99-3199-47da-aaf5-c8359f48a6e8"
    },
    {
      "key": "harvest_source_id",
      "value": "39e4ad2a-47ca-4507-8258-852babd0fd99"
    },
    {
      "key": "harvest_source_title",
      "value": "NASA Data.json"
    }
  ],
  "license_url": "http://www.usa.gov/publicdomain/label/1.0/",
  "title": "Efficient Matlab Programs",
  "revision_id": "d33a7f14-6103-4bb8-b450-9355853137cc"
}